{
  "K-Planes: Explicit Radiance Fields in Space, Time, and Appearance": {
    "title": "K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",
    "authors": [
      "Sara Fridovich-Keil",
      "Giacomo Meanti",
      "Frederik Warburg",
      "Benjamin Recht",
      "Angjoo Kanazawa"
    ],
    "abstract": "We introduce k-planes, a white-box model for radiance fields in arbitrary\ndimensions. Our model uses d choose 2 planes to represent a d-dimensional\nscene, providing a seamless way to go from static (d=3) to dynamic (d=4)\nscenes. This planar factorization makes adding dimension-specific priors easy,\ne.g. temporal smoothness and multi-resolution spatial structure, and induces a\nnatural decomposition of static and dynamic components of a scene. We use a\nlinear feature decoder with a learned color basis that yields similar\nperformance as a nonlinear black-box MLP decoder. Across a range of synthetic\nand real, static and dynamic, fixed and varying appearance scenes, k-planes\nyields competitive and often state-of-the-art reconstruction fidelity with low\nmemory usage, achieving 1000x compression over a full 4D grid, and fast\noptimization with a pure PyTorch implementation. For video results and code,\nplease see https://sarafridov.github.io/K-Planes.",
    "page_id": "823267e4-6554-45dd-a53e-0f09d4f326f5"
  },
  "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis": {
    "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
    "authors": [
      "Ben Mildenhall",
      "Pratul P. Srinivasan",
      "Matthew Tancik",
      "Jonathan T. Barron",
      "Ravi Ramamoorthi",
      "Ren Ng"
    ],
    "abstract": "We present a method that achieves state-of-the-art results for synthesizing\nnovel views of complex scenes by optimizing an underlying continuous volumetric\nscene function using a sparse set of input views. Our algorithm represents a\nscene using a fully-connected (non-convolutional) deep network, whose input is\na single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing\ndirection $(\\theta, \\phi)$) and whose output is the volume density and\nview-dependent emitted radiance at that spatial location. We synthesize views\nby querying 5D coordinates along camera rays and use classic volume rendering\ntechniques to project the output colors and densities into an image. Because\nvolume rendering is naturally differentiable, the only input required to\noptimize our representation is a set of images with known camera poses. We\ndescribe how to effectively optimize neural radiance fields to render\nphotorealistic novel views of scenes with complicated geometry and appearance,\nand demonstrate results that outperform prior work on neural rendering and view\nsynthesis. View synthesis results are best viewed as videos, so we urge readers\nto view our supplementary video for convincing comparisons.",
    "page_id": "f3b8c51c-e69f-4cf5-8219-ba6deae3009d"
  },
  "Learning Neural Duplex Radiance Fields for Real-Time View Synthesis": {
    "title": "Learning Neural Duplex Radiance Fields for Real-Time View Synthesis",
    "authors": [
      "Ziyu Wan",
      "Christian Richardt",
      "Alja\u017e Bo\u017ei\u010d",
      "Chao Li",
      "Vijay Rengarajan",
      "Seonghyeon Nam",
      "Xiaoyu Xiang",
      "Tuotuo Li",
      "Bo Zhu",
      "Rakesh Ranjan",
      "Jing Liao"
    ],
    "abstract": "Neural radiance fields (NeRFs) enable novel view synthesis with unprecedented\nvisual quality. However, to render photorealistic images, NeRFs require\nhundreds of deep multilayer perceptron (MLP) evaluations - for each pixel. This\nis prohibitively expensive and makes real-time rendering infeasible, even on\npowerful modern GPUs. In this paper, we propose a novel approach to distill and\nbake NeRFs into highly efficient mesh-based neural representations that are\nfully compatible with the massively parallel graphics rendering pipeline. We\nrepresent scenes as neural radiance features encoded on a two-layer duplex\nmesh, which effectively overcomes the inherent inaccuracies in 3D surface\nreconstruction by learning the aggregated radiance information from a reliable\ninterval of ray-surface intersections. To exploit local geometric relationships\nof nearby pixels, we leverage screen-space convolutions instead of the MLPs\nused in NeRFs to achieve high-quality appearance. Finally, the performance of\nthe whole framework is further boosted by a novel multi-view distillation\noptimization strategy. We demonstrate the effectiveness and superiority of our\napproach via extensive experiments on a range of standard datasets.",
    "page_id": "beb1a75b-85dd-4715-9237-2f544f9d43a7"
  }
}